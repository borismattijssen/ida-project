---
title: "Homework 3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

We present a brief study of a time series representing new housing approvals.
It has monthly data from January 1997 to August 2013. [ Source: Banco de Espana - www.bde.es ]

```{r, warning=FALSE,message=FALSE}
library("astsa")
library("forecast")
library("fpp")
library("fpp2")
library("gridExtra")
library("ggplot2")
library("knitr")
library("kableExtra")
library("car")
```

# 1.
### Time series description

```{r time series plot}
df=readxl::read_xlsx("data_g12.xlsx", sheet = 1)
house.ts <- ts(df[,2], frequency = 12,start = c(1997,1))
autoplot(house.ts)
```

We can immediatly notice that the behaviour of the series changes drastically during year 2007. 

The trend in the first part is slightly increasing, almost linearly, while in the second part it is strongly descending.

The seasonality is more evident in the first part, while in the second part is harder to notice any seasonality at first sight, due to the different order of magnitude of the values, but could be still present.

```{r seasonality, fig.height = 3, fig.width = 8, echo=FALSE}
p1=ggmonthplot(house.ts)
p2=ggseasonplot(house.ts)
grid.arrange(p1, p2, ncol = 2)
```

There is not a constant variance in the series. We can notice higher variance in the central part, while the beginning (up to 2003) and the "tail" have lower variance.

The first part seems to have a cyclical component. There are rises and falls barely visible, larger than the usual seasonal variations. A deeper inspection told us that the lowest frequency with the maximum amplitude is at ```fr=0.06```. 
The estimated length of the cycle is then ```1/0.06 = 16.7```. 
The pattern repeats approximately every 16.7 months.

```{r cycle checking, results='hide', fig.show='hide'}
which.max(spec.pgram(house.ts)$spec) # = 1
spec.pgram(house.ts)$freq[1]         # = 0.06
```

```{r periodogram, fig.height = 4, fig.width =5, echo=FALSE}
spec.pgram(house.ts)
```

After all these considerations, we consider the time series as not stationary.

# 2.
### Seasonal and Trend decomposition
 
We use a method based on local weighted regression (loess) for extracting the trends and the seasonality of the series.
Since the series has a non constant variance, we use a multiplicative decomposition.
The method ``` stl()``` only offers an additive decomposition. We can obtain a multipicative one, by applying a logarithmic transformation to the original series.

$$Y_t = S_t \times T_t \times E_t \longmapsto \log Y_t = \log S_t + \log T_t + \log E_t$$

```{r stl decomposition}
house.ts <- house.ts[,1]
house.ts.log <- log(house.ts)
stl.house <- stl(house.ts.log, s.window = "periodic", robust = TRUE)
plot(stl.house$time.series)
```

### Forecasting

We use three different methods for predicting future values of the series.
Then we use a cross validation for predicting the error.

```{r forecast plot}
fcst=forecast(stl.house, method="ets", h=12)
p1=autoplot(fcst)+ ylab("")
fcst=forecast(stl.house, method="arima", h=12)
p2=autoplot(fcst)
fcst=forecast(stl.house, method="rwdrift", h=12)
p3=autoplot(fcst) + ylab("")
grid.arrange(p1,p2,p3)
```

```{r cross validation}
stl.house.cv <- function(x, h, m){forecast(stl(x,s.window = "periodic", robust = TRUE), method=m, h=h)}
error.ets <- tsCV(house.ts.log, stl.house.cv,m="ets", h=12)
error.arima <- tsCV(house.ts.log, stl.house.cv,m="arima", h=12)
error.rwd <- tsCV(house.ts.log, stl.house.cv, m="rwdrift", h=12)
list.error <- data.frame(sqrt(mean(error.ets^2, na.rm=TRUE)), 
                sqrt(mean(error.arima^2, na.rm=TRUE)),
                sqrt(mean(error.rwd^2, na.rm=TRUE)), row.names = "RMSE")
colnames(list.error) <- c("ets","arima","rwdrift")
kable(list.error) %>% kable_styling(full_width = F)
```

The first method used ```("ets")``` seems to be the best one, as it shows a lower value of RMS error.


### Residuals

Let's check the residuals distribution. It remaind us a normal shape distribution with zero mean, but unconstant variance.
Furthermore, the ACF plot enhance a couple of significant spike, suggesting correlation between the residuals. This correlaiton is also confirmed in the periodogram, that underline a cyclic behaviour.
We can not assume, therefore, that the residuals are independents.
```{r residuals}
tsdisplay(stl.house$time.series[,3], plot.type="spectrum")
```



#3
###A)Deciding the proper transformation to apply

```{r}
df=readxl::read_xlsx("data_g12.xlsx", sheet = 1)
house.ts <- ts(df[,2], frequency = 12,start = c(1997,1))
plot(house.ts)
plot(log(house.ts))
sd(house.ts)
sd(log(house.ts))
```

Logarithmic and box-cox transformation seems more or less the same, in both the variance compared to the untransformed variable is less.

```{r}
tsdisplay(house.ts)
tsdisplay(log(house.ts))
tsdisplay(BoxCox(log(house.ts), lambda = BoxCox.lambda(house.ts)))
```

We decide to use logarithmic transformation since the standard deviation is minor.

###B)Checking if there is seasonality

```{r Periodgram}
spec.pgram(log(house.ts))
```

Does not seem to occur seasonality since there are no visible patterns in the spectrogram.


```{r ACF and PACF}
acf2(log(house.ts))
```

Since the ACF slowly goes to zero, and in the PACF there is only one initial spike and then the vaues'modulus decrease to zero, the series is not stationary. 


```{r seasonplot and monthplot}
ggseasonplot(log(house.ts))
ggmonthplot(log(house.ts))
```

Appears from both graphics to be a drop in august, probably due to holidays. But
there is no seasonality since all the months look the same in the monthplot.

###C) Decide the value of order of differentation
The seires does not appear stationary to a visual inspection, we have a further proof from the two following tests:
```{r KPSS Test for Stationarity}
kpss.test(log(house.ts))
```

H0:stationarity, we refuse stationarity.

```{r Augmented Dickey Fuller Test}
adf.test(log(house.ts),alternative = "stationary")
```

H0:non-stationarity, we accept non-stationarity.

```{r standard deviation}
sd(log(house.ts))
```

Since our series is not stationary and ARIMA only works with stationary series we try to make it stanionary by applying some transformations.

###differencing once
We try differencing once:

```{r one differentiation}
plot(diff(log(house.ts)))
```
```{r 1 diff ACF and PACF}
acf2(diff(log(house.ts)))
```

```{r 1 diff KPSS Test for Stationarity}
kpss.test(diff(log(house.ts)))
```

H0:stationarity, we refuse stationarity.

```{r 1 diff Augmented Dickey Fuller Test}
adf.test(diff(log(house.ts)),alternative = "stationary")
```

H0:non-stationarity we refuse non stationarity.


```{r 1 diff Phillips Perron Unit Root Test}
pp.test(diff(BoxCox(log(house.ts), lambda = BoxCox.lambda(house.ts)),alternative = "stationary"))
```

H0:non-stationarity, we refuse non stationarity.

```{r 1 diff standard deviation}
sd(diff(log(house.ts)))
```

Standard deviation strongly decrease from before than differencing.

From the rough plot of the series can appear stationary,but one test is telling us it is not stationary and also ACF plot's spikes can be a sign of non-stationary.

###differencing twice
Lets try differencing twice.

```{r two differentiations}
plot(diff(diff(log(house.ts))))
```
```{r 2 diff ACF and PACF}
acf2(diff(diff(log(house.ts))))
```

```{r 2 diff KPSS Test for Stationarity}
kpss.test(diff(diff(log(house.ts))))
```

H0:stationarity, we accept stationarity.

```{r 2 diff Augmented Dickey Fuller Test}
adf.test(diff(diff(log(house.ts),alternative = "stationary")))
```

H0:non-stationarity,we refuse non-stationarity.

```{r 2 diff Phillips Perron Unit Root Test}
pp.test(diff(diff(log(house.ts),alternative = "stationary")))
```

H0:non-stationarity,we refuse non-stationarity.

```{r 2 diff standard deviation}
sd(diff(diff(log(house.ts))))
```

The series differenced twice is finally stationary.

Final check
```{r}
ndiffs(log(house.ts),alpha = 0.05)
```

We can conclude that the proper value of D is D=2.

3.d)
p,q, P and Q trying to interpret acf and pacf
```{r}
model.1=Arima(log(house.ts),order=c(2,1,0),seasonal=list(order=c(0,1,1), period=12))
model.2=Arima(log(house.ts),order=c(2,1,0),seasonal=list(order=c(3,2,0), period=12))
model.3=Arima(log(house.ts),order=c(2,1,0),seasonal=list(order=c(3,3,1), period=12))
model.4=Arima(log(house.ts),order=c(0,1,4),seasonal=list(order=c(3,1,0), period=12))
model.5=Arima(log(house.ts),order=c(0,1,4),seasonal=list(order=c(0,2,1), period=12))
```

Based on the AICc, the winner is model.1 but it doesn't seem to validate the assumptions residuals.
Now let's check the correlation between the coefficient of the model:

```{r}
cov2cor(model.1$var.coef)
```

There is few correlation between the coefficient of the model.1 which is good.

3.e) Below, the diagnostic of the residuals for the final model.1: 
```{r}
plot(model.1$residuals)
t.test(model.1$residuals)
Box.test(model.1$residuals, lag=12, type="L")
jarque.bera.test(model.1$residuals)
which.max(model.1$residuals)
jarque.bera.test(model.1$residuals[-117])
```


### 3f
To forecast the timeseries we use the `forecast()` function for `model.1`.

```{r}
plot(forecast(model.1, h=12))
```

### 3g
To estimate the RMSE we first define the `getrmse` function. Also, a function `getrmse.bymodel` is defined that extracts the ARIMA parameters from a model and feeds it to the `getrmse` function.

```{r}
# Cross-validation 
# function getrmse() from https://www.otexts.org/fpp/8/9
# With this function you use the data but the last 12 observations as the training set, 
# and use the last 12 observations to test the model and compute RMSE (and other measures of accuracy)
# with these 12 observations. 
getrmse <- function(x,h,...)
{
  train.end <- time(x)[length(x)-h]   #train data end
  test.start <- time(x)[length(x)-h+1]  #test data start
  train <- window(x,end=train.end) #extract train data
  test <- window(x,start=test.start)  #extract test data
  fit <- Arima(train,...) # fit model with train data
  fc <- forecast(fit,h=h) # forecast with model
  return(accuracy(fc,test)[2,"RMSE"]) #compare forecast with test data, extract the rmse
}
# Extract ARIMA parameters from the model and call `getrmse` accordingly.
getrmse.bymodel <- function(model,h)
{
  arma = model$arma
  p = arma[1]
  d = arma[6]
  q = arma[2]
  P = arma[3]
  D = arma[7]
  Q = arma[4]
  return(getrmse(model$x,h=h,order=c(p,d,q),seasonal=c(P,D,Q)))
}
```

Now, we compare the RMSEs for the models defined above. 

```{r}
rmse1 = getrmse.bymodel(model.1, h=12)
rmse2 = getrmse.bymodel(model.2, h=12)
rmse3 = getrmse.bymodel(model.3, h=12)
rmse4 = getrmse.bymodel(model.4, h=12)
rmse5 = getrmse.bymodel(model.5, h=12)
c(rmse1, rmse2, rmse3, rmse4, rmse5)
```

Here we see that `model.1` has the best predictive power because it has the lowest RMSE.

### 3h
We will have a look at the `auto.arima()` function to see if it can find a better parameter set.

```{r}
model.auto=auto.arima(house.ts)
model.auto
```

A model was found with (p,d,q) = (1,1,2) and (P,D,Q) = (1,0,1). The AIC is worse than our manual selected model, but it has better predictive power:

```{r}
getrmse.bymodel(model.auto,h=12)
```

Problems, however, arise when we look at the residuals and model parameters. First, let us look at the correlation between model parameters:

```{r}
cov2cor(model.auto$var.coef)
```

We see that `sar1` and `sma1` have a correlation higher than 0.8. This is bad. Now, secondly, let us look at the residuals. They should be independently identically and normally distributed. This is what the residuals look like:

```{r}
plot(model.auto$residuals)
```

We now use the qq-plot to get an impression of whether normality is fulfilled and to determine outliers.

```{r}
model.auto.outliers = qqPlot(model.auto$residuals)
```

The data does not look normally distributed. Let us remove the outliers and test for the above stated requirements.

```{r}
model.auto.res.noo = model.auto$residuals[-c(117,119)]
# H0: The data are independently distributed 
Box.test(model.auto.res.noo, lag=12, type="Ljung-Box", fitdf=3)
```

The Ljung-Box test tells us that the residuals are not independently distributed.

```{r}
# H0: The mean is 0
t.test(model.auto.res.noo)
```

The Student's t-test tells us that we can accept the residuals mean to be 0. 

```{r}
# H0: Skewness and kurtosis are both 0
jarque.bera.test(model.auto.res.noo)
```

The Jarque Bera test tells us that we cannot accept that the skewness and kurtosis of the residuals are both 0 (like the normal distribution).

Also, when looking at the frequency spectrum of the residuals, we can see that certain frequencies are more present. An independent process is expected to have equal presence of all frequencies. Hence, the residuals are not independently distribured.
```{r}
# The residuals seem to have some frequency component in there (should be white noise band).
tsdisplay(model.auto.res.noo, plot.type="spectrum")
```

We conclude that the automatically selected ARIMA model does not suffice and we will therefore advise to select our manually chosen model `model.1`.
