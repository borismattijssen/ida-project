---
title: "Clustering"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning=FALSE,message=FALSE}
library(ggplot2)
library(GGally)
library(FactoMineR)
library(factoextra)
library(dplyr)
library(tidyr)
library(cluster)
```


```{r,  warning=FALSE}

# loading and extracting the 5 features
cars <- read.table("cars.txt", header=TRUE, as.is=TRUE)
cars <- na.omit(cars)
rownames(cars) <- abbreviate(make.unique(cars$name))
cars$origin <- as.factor(cars$origin)
cars.pc <- select(cars, mpg, cylinders, displacement, horsepower, weight, acceleration, year, origin)
cars.pca=PCA(cars.pc,quali.sup=8,ncp=5,scale.unit=TRUE, graph=FALSE)

# five principal component scores:
car5 <- cars.pca$ind$coord
car5_scale <- scale(car5, center=FALSE)


# quick view of the 5 variable distributions 
ggplot(gather(as.data.frame(car5)), aes(value)) + 
  geom_histogram(bins = 25) + 
  aes(fill = as.factor(key)) + 
  facet_wrap(~key) +
  labs(fill= "Components") +
  ggtitle('NOT normalized dimensions')


# after normalization
ggplot(gather(as.data.frame(car5_scale)), aes(value)) + 
  geom_histogram(bins = 25) + 
  aes(fill = as.factor(key)) + 
  facet_wrap(~key) +
  labs(fill= "Components") +
  ggtitle('normalized dimensions')

```

As expected, the values of the first two dimensions are more spread with an higher (emphirical) variance, and they are going to dominate the cluster, specially Dim.2
Once scaled, this behaviour is reduced.



### Kmeans clustering

#####(k=3)


```{r, warning=FALSE}

set.seed(12345)

#k=3:
car5.k3 <- kmeans(car5_scale, centers=3, iter.max=100, nstart=25)
pairs(car5_scale, col=car5.k3$cluster)

```

From this first plot, is evident that the cluster division has followed more the distribution of the first two dimensions. The scatter plot of the first two variables, indeed, shows how the clusters division avoid quite well any overlapping, and divide point belonging to different areas of the 2-D space. A neat separation of the clusters it's not present in any other different combination of dimensions.

### Matching the clusters with the Origins class


```{r, warning=FALSE}

ggplot(as.data.frame(cars$origin)) + 
  geom_histogram(bins = 3, stat = "count", aes(x = cars$origin, fill=cars$origin))+
  labs(fill= "Class origins") +
  scale_fill_brewer() +
  ggtitle('class origins distribution')
```

Before looking at how the cluster represent this 3 groups, let's have a look at the distribution of them in term of cardinality. The three groups are not balanced.

```{r, warning=FALSE}


#first 2 component visualization
fviz_cluster(object= car5.k3, data=car5_scale, pointsize=4, geom="point", shape=cars$origin,
             ellipse.alpha = 0.05, show.clust.cent = F, stand =F,
             choose.vars = c("Dim.1", "Dim.2"))+
  labs(color="clusters") +
  theme_minimal()

```

While the colors represent the cluster partitions, the shape of the points represent the class `origins`.
It's quite evident that our cluster partiotion did not work well for representing this information.

Cluster one is "pure", containing only (but not all the) element of the first class, which is the more relevant in term of cardinality. It also do not overlap in space with the other two clusters.

The third cluster is also dominated by the presence of element from first class, but records from different classes are present. 

Finally the third one is quite mixed. There's also a small region of space shared with the second cluster.



### Matching the clusters with the Cylinders class 


```{r, warning=FALSE}

#DELETE the observation with 3 cylinders
car5_98 <- car5[ !(rownames(car5) %in% "mr2c"), ]
cars_98 <- cars[ !(rownames(cars) %in% "mr2c"), ]
cars_98$cylinders <- as.factor(cars_98$cylinders)
car5_98_scale <- scale(car5_98, center=FALSE)

ggplot(as.data.frame(cars_98$cylinders)) + 
  geom_histogram(bins = 3, stat = "count", aes(x = cars_98$cylinders, fill=cars_98$cylinders))+
  labs(fill= "Number of cylinders") +
  scale_fill_brewer() +
  ggtitle('class cylinders distribution')
```

As suggested, we drop the records in the dataset containg 3 cylinders. The cardinality of these groups is more balanced.

```{r, warning=FALSE}

car5_98.k3 <- kmeans(car5_98_scale, centers=3, iter.max=100, nstart=25)

#first 2 component visualization
fviz_cluster(object= car5_98.k3, data=car5_98, pointsize=4, geom="point",
             shape=cars_98$cylinders,
             ellipse.alpha = 0.05, show.clust.cent = F, stand =F,
             choose.vars = c("Dim.1", "Dim.2"))+
  labs(color="cluster") +
  theme_minimal()
```

The clustering partion now is more coherent with the class cylinders. 
On the left we have cluster two containing all and only records belonging to first class (cylinders = 4).
In the middle, cluster three contains both elements from class two (cylinders = 6) and three (cylinders = 8), while cluster one surrounds the remaing element, all belonging to class three. Notice that the cluster division works better also because the class are already separated in space, according to the first two dimesions we are plotting.



#### LUMP and SPLIT errors


#####function definition

```{r}
count.errors <- function (class, cluster, error) {
  matrix.class.row <- matrix(class,length(class),length(class), byrow = T)
  matrix.class.col <- matrix(class,length(class),length(class), byrow = F)
  matrix.class.same <- matrix.class.col==matrix.class.row
  
  matrix.cluster.row <- matrix(cluster,length(cluster),length(cluster), byrow = T)
  matrix.cluster.col <- matrix(cluster,length(cluster),length(cluster), byrow = F)
  matrix.cluster.same <- matrix.cluster.col==matrix.cluster.row
  
  if (error == "lump") {
    matrix.lump <- matrix.cluster.same==T & matrix.class.same==F
    n.lump <- sum(matrix.lump==T) / 2
    return(n.lump)
  } else { 
    if (error == "split") {
      matrix.split <- matrix.class.same==T & matrix.cluster.same==F
      n.split <- sum(matrix.split==T) / 2
      return(n.split)
    } else {
      print (" USAGE: in error specify either \"lump\" or \"split\" ")
    }
  }
}

```

##### lump and spit errors for class Origins and Cylinders

```{r}
cat( "lump error with class origins: ", count.errors(cars$origin, car5.k3$cluster, "lump"), "\n")
cat( "split error with class origins: ", count.errors(cars$origin, car5.k3$cluster, "split"), "\n")
cat( "lump error with class cylinders: ", count.errors(cars_98$cylinders, car5_98.k3$cluster, "lump"), "\n")
cat( "split error with class cylinders: ", count.errors(cars_98$cylinders, car5_98.k3$cluster, "split"), "\n")
```
It's quite clear that in term of these two measure, lump and split errors, the clustering method represent better the class cylinders than the class origins.

We can notice an high splitting error related to the class `origins`, this means that a lot of records belonging to the same class have been assinged to different clusters. Such high error is due to the fact that records from class one, has been splitted between all the three clusters.



