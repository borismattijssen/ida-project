---
title: "Homework 1.3"
output:
  html_document:
    df_print: paged
---

<style>
  .superbigimage{
      overflow-x:scroll;
      white-space: nowrap;
  }

  .superbigimage img{
     max-width: none;
  }


</style>

## Introduction

## The cars dataset
In this section we are going to perform a principal component analysis on the cars dataset. After the analysis we will use the transformed data in combination with a k-mean clustering algorithm to try to predict the origin of a car. 
```{r, warning=FALSE,message=FALSE}
library(knitr)
library(gridExtra)
library(dplyr)
library(ggplot2)
library(GGally)
library(FactoMineR)
library(factoextra)
```

Here, we load the data and give unique names to each row based on the name of the car.
```{r, results='asis', echo=TRUE}
cars <- read.table("cars.txt", header=TRUE, as.is=TRUE)
cars <- na.omit(cars)
rownames(cars) <- abbreviate(make.unique(cars$name))
cars$origin <- as.factor(cars$origin)
kable(cars[1:5,])
```

### Principal component analysis
We perform the PCA by selecting 5 output variables and taking the car's origin as a supplementary variable. Hence, this variable will not be used for finding the principal components, but could later be used as additional information for plots and analysis.
```{r}
cars.pc <- select(cars, mpg, cylinders, displacement, horsepower, weight, acceleration, year, origin)
cars.pca=PCA(cars.pc,quali.sup=8,ncp=5,scale.unit=TRUE, graph=FALSE)
```

#### Analysis of the principal components
First, we look at the variances explained by the different prinicipal components:
```{r}
cars.pca$eig
```

From this table we see that most variance is explained by the first principle component (PC) (71.59%), while the second and third PC also contribute some variance. This can also be visually observed with a scree plot.
```{r}
plot(cars.pca$eig[,1], type="l")
points(cars.pca$eig[,1])
```

#### Analysis of the individual observations
A plot of the first two principal components, grouped by the car's origin, looks as follows.
```{r}
fviz_pca_ind(cars.pca,  label="none", habillage = "origin", addEllipses=TRUE, ellipse.level=0.95)
```

From this plot we see that in the first two principal components we can already separate the class `origin=1` pretty well, but the other two classes are still overlapping. 

Now, let us find out which individuals contribute most to the first principle component.
```{r}
contrib.sorted <- sort(cars.pca$ind$contrib[,1],decreasing = TRUE)
indcontrib <- data.frame(C1=contrib.sorted,n=as.factor(names(contrib.sorted)))
G1 <- ggplot(indcontrib,aes(x=reorder(n,-C1), y=C1)) +
  geom_bar(position=position_dodge(), stat="identity", fill="steelblue") +
  geom_text(aes(label=n), vjust=1.6, color="white", size=2.5, position=position_dodge(0.9)) +
  geom_hline(yintercept=100/74)
# the graph below is horizonally scrollable
```
<div class="superbigimage">
```{r plot_it, fig.width=30,fig.height=3, echo=FALSE}
G1
```
</div>

#### Analysis of the variables
To understand the relations between the original variables and the PCA'd variables we can look at their correlations. Strong correlations imply that the PCA'd variable represents the original variable well. These correlations are nicely visualized with a **circle of correlations**. Here we show the circle of correlations for the first two PCA dimensions.

```{r}
fviz_pca_var(cars.pca, col.var="contrib")
```

What we see is that `weight`, `cylinders`, `displacement`, and `horsepower` have high correlations with Dimension 1. These are also the variables that contribute the most to the representation. This makes sense, because Dimension 1 explains a lot of the variance of the original dataset. 

The total quality of the variables in these two dimensions is shown below. Variables have higher quality if they align well with one of two dimensions and have a longer distance to the origin.
```{r}
cars.pca$var$cos2[,1]+ cars.pca$var$cos2[,2]
```

Below we show the contribution of the different variables to the different dimension. Some observation are: (i) `displacement` and `weight` do not contribute particularly much to a specific dimension, hence they are probably a linear combination of the other variables, and (ii) Dimension 1 has a rather uniform distribution over the variables. 
```{r plot_it2, fig.width=12,fig.height=4}
var.contrib    <- cars.pca$var$contrib
my.grid        <- expand.grid(x=rownames(var.contrib), y=colnames(var.contrib))
my.grid$values <- as.vector(var.contrib)
G1 <- ggplot(my.grid, aes(x=x, y=values))+geom_bar(stat="identity", aes(fill=y), position=position_dodge())
G2 <- ggplot(my.grid, aes(x=y, y=values))+geom_bar(stat="identity", aes(fill=x), position=position_dodge())
grid.arrange(G1,G2,ncol=2)
```

#### Variables and individuals
The first two principal components explain most of the variance in the data (87.5%). Looking at a 2D-plot of the transformed data points, therefore, gives us a good intuition of similarity between cars. Furthermore, we can show the loading of each variable in the same plot. This could give us an indication as of why these cars are clustered together. For example, the cars `plag`, `boid`, and `frdp` have higher acceleration than `bs32`, `amad`, and `bew(`, because the first three lie in the direction of the `acceleration` arrow.

```{r plot_it3, fig.width=12, fig.height=12}
fviz_pca_biplot(cars.pca)
```