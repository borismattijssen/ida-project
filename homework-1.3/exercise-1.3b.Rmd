---
title: "Homework 1.3"
output:
  html_document:
    df_print: paged
---

<style>
  .superbigimage{
      overflow-x:scroll;
      white-space: nowrap;
  }

  .superbigimage img{
     max-width: none;
  }


</style>

## Introduction

## The cars dataset
In this section we are going to perform a principal component analysis on the cars dataset. After the analysis we will use the transformed data in combination with a k-mean clustering algorithm to try to predict the origin of a car. 
```{r, warning=FALSE,message=FALSE}
library(knitr)
library(gridExtra)
library(dplyr)
library(ggplot2)
library(GGally)
library(FactoMineR)
library(factoextra)
library(tidyr)
library(cluster)
```

Here, we load the data and give unique names to each row based on the name of the car.
```{r, results='asis', echo=TRUE}
cars <- read.table("cars.txt", header=TRUE, as.is=TRUE)
cars <- na.omit(cars)
rownames(cars) <- abbreviate(make.unique(cars$name))
cars$origin <- as.factor(cars$origin)
kable(cars[1:5,])
```

### Principal component analysis
We perform the PCA by selecting 5 output variables and taking the car's origin as a supplementary variable. Hence, this variable will not be used for finding the principal components, but could later be used as additional information for plots and analysis.
```{r}
cars.pc <- select(cars, mpg, cylinders, displacement, horsepower, weight, acceleration, year, origin)
cars.pca=PCA(cars.pc,quali.sup=8,ncp=5,scale.unit=TRUE, graph=FALSE)
```

#### Analysis of the principal components
First, we look at the variances explained by the different prinicipal components:
```{r}
kable(cars.pca$eig)
```

From this table we see that most variance is explained by the first principle component (PC) (71.59%), while the second and third PC also contribute some variance. This can also be visually observed with a scree plot.
```{r}
plot(cars.pca$eig[,1], type="l")
points(cars.pca$eig[,1])
```

#### Analysis of the individual observations
A plot of the first two principal components, grouped by the car's origin, looks as follows.
```{r}
fviz_pca_ind(cars.pca,  label="none", habillage = "origin", addEllipses=TRUE, ellipse.level=0.95)
```

From this plot we see that in the first two principal components we can already separate the class `origin=1` pretty well, but the other two classes are still overlapping. 

Now, let us find out which individuals contribute most to the first principle component.
```{r}
contrib.sorted <- sort(cars.pca$ind$contrib[,1],decreasing = TRUE)
indcontrib <- data.frame(C1=contrib.sorted,n=as.factor(names(contrib.sorted)))
G1 <- ggplot(indcontrib,aes(x=reorder(n,-C1), y=C1)) +
  geom_bar(position=position_dodge(), stat="identity", fill="steelblue") +
  geom_text(aes(label=n), vjust=1.6, color="white", size=2.5, position=position_dodge(0.9)) +
  geom_hline(yintercept=100/74)
# the graph below is horizonally scrollable
```
<div class="superbigimage">
```{r plot_it, fig.width=30,fig.height=3, echo=FALSE}
G1
```
</div>

#### Analysis of the variables
To understand the relations between the original variables and the PCA'd variables we can look at their correlations. Strong correlations imply that the PCA'd variable represents the original variable well. These correlations are nicely visualized with a **circle of correlations**. Here we show the circle of correlations for the first two PCA dimensions.

```{r}
fviz_pca_var(cars.pca, col.var="contrib")
```

What we see is that `weight`, `cylinders`, `displacement`, and `horsepower` have high correlations with Dimension 1. These are also the variables that contribute the most to the representation. This makes sense, because Dimension 1 explains a lot of the variance of the original dataset. 

The total quality of the variables in these two dimensions is shown below. Variables have higher quality if they align well with one of two dimensions and have a longer distance to the origin.
```{r}
cars.pca$var$cos2[,1]+ cars.pca$var$cos2[,2]
```

Below we show the contribution of the different variables to the different dimension. Some observation are: (i) `displacement` and `weight` do not contribute particularly much to a specific dimension, hence they are probably a linear combination of the other variables, and (ii) Dimension 1 has a rather uniform distribution over the variables. 
```{r plot_it2, fig.width=12,fig.height=4}
var.contrib    <- cars.pca$var$contrib
my.grid        <- expand.grid(x=rownames(var.contrib), y=colnames(var.contrib))
my.grid$values <- as.vector(var.contrib)
G1 <- ggplot(my.grid, aes(x=x, y=values))+geom_bar(stat="identity", aes(fill=y), position=position_dodge())
G2 <- ggplot(my.grid, aes(x=y, y=values))+geom_bar(stat="identity", aes(fill=x), position=position_dodge())
grid.arrange(G1,G2,ncol=2)
```

#### Variables and individuals
The first two principal components explain most of the variance in the data (87.5%). Looking at a 2D-plot of the transformed data points, therefore, gives us a good intuition of similarity between cars. Furthermore, we can show the loading of each variable in the same plot. This could give us an indication as of why these cars are clustered together. For example, the cars `plag`, `boid`, and `frdp` have higher acceleration than `bs32`, `amad`, and `bew(`, because the first three lie in the direction of the `acceleration` arrow.

```{r plot_it3, fig.width=12, fig.height=12}
fviz_pca_biplot(cars.pca)
```

### K-means clustering

Here, we use the 5 principal components extracted in the previous steps, and we use them for clustering the dataset. Later on, we inspect how the clustered groups follow the distributions of two categorical variables of the original dataset, `Origins` and `Cylinders`. 

```{r,  warning=FALSE}

# five principal component scores:
car5 <- cars.pca$ind$coord
car5_scale <- scale(car5, center=FALSE)


# quick view of the 5 variable distributions 
ggplot(gather(as.data.frame(car5)), aes(value)) + 
  geom_histogram(bins = 25) + 
  aes(fill = as.factor(key)) + 
  facet_wrap(~key) +
  labs(fill= "Components") +
  ggtitle('NOT normalized dimensions')


# after normalization
ggplot(gather(as.data.frame(car5_scale)), aes(value)) + 
  geom_histogram(bins = 25) + 
  aes(fill = as.factor(key)) + 
  facet_wrap(~key) +
  labs(fill= "Components") +
  ggtitle('normalized dimensions')

```

As expected, the values of the first two dimensions are more spread with an higher (empirical) variance, and they are going to dominate the cluster, specifically Dimension 2.
Once scaled, this behaviour is reduced.

#####(k=3)

The algorithm used for clustering, in this case, is a partitioning clustering, based on centroids.
We specify the number of clusters we want to obtain (three), and we also fix a seed, as suggested, for replicability, since the algorithm gets his inizialization randomly.

```{r, warning=FALSE}

set.seed(12345)

#k=3:
car5.k3 <- kmeans(car5_scale, centers=3, iter.max=100, nstart=25)
pairs(car5_scale, col=car5.k3$cluster)

```

This is a matrix of scatterplots, representing all the combination of variables, two by two, for visualizing our 5-dimensions clusterized objects in 2D
From this first plot, it is evident that the cluster division has followed mainly the distribution of the first two dimensions. The scatter plot of the first two variables shows how the clusters are almost non-overlapping. A neat separation of the clusters is not present in any other combination of dimensions.

### Matching the clusters with the Origins class


```{r, warning=FALSE}

ggplot(as.data.frame(cars$origin)) + 
  geom_histogram(bins = 3, stat = "count", aes(x = cars$origin, fill=cars$origin))+
  labs(fill= "Class origins") +
  scale_fill_brewer() +
  ggtitle('class origins distribution')
```

Before looking at how the cluster represent this 3 groups, let's have a look at the distribution of them in term of cardinality. The three groups are not balanced.

```{r, warning=FALSE}


#first 2 component visualization
fviz_cluster(object= car5.k3, data=car5_scale, pointsize=4, geom="point", shape=cars$origin,
             ellipse.alpha = 0.05, show.clust.cent = F, stand =F,
             choose.vars = c("Dim.1", "Dim.2"))+
  labs(color="clusters") +
  theme_minimal()

```

While the colors represent the cluster partitions, the shape of the points represent the class `origins`.
It's quite evident that our cluster partiotion did not work well for representing this information.

Cluster one (red) is "pure", containing only (but not all the) element of the first class (circle), which is the more relevant in term of cardinality. It also do not overlap in space with the other two clusters.

The third cluster (blu) is also dominated by the presence of element from first class, but records from different classes are present. 

Finally the second one (green) is quite mixed. There's also a small region of space shared with the second cluster, due to the fact that we are using only the first two dimensions for plotting.



### Matching the clusters with the Cylinders class 


```{r, warning=FALSE}

#DELETE the observation with 3 cylinders
car5_98 <- car5[ !(rownames(car5) %in% "mr2c"), ]
cars_98 <- cars[ !(rownames(cars) %in% "mr2c"), ]
cars_98$cylinders <- as.factor(cars_98$cylinders)
car5_98_scale <- scale(car5_98, center=FALSE)

ggplot(as.data.frame(cars_98$cylinders)) + 
  geom_histogram(bins = 3, stat = "count", aes(x = cars_98$cylinders, fill=cars_98$cylinders))+
  labs(fill= "Number of cylinders") +
  scale_fill_brewer() +
  ggtitle('class cylinders distribution')
```

As suggested, we drop the records in the dataset containg 3 cylinders. The cardinality of these groups is more balanced.

```{r, warning=FALSE}

car5_98.k3 <- kmeans(car5_98_scale, centers=3, iter.max=100, nstart=25)

#first 2 component visualization
fviz_cluster(object= car5_98.k3, data=car5_98, pointsize=4, geom="point",
             shape=cars_98$cylinders,
             ellipse.alpha = 0.05, show.clust.cent = F, stand =F,
             choose.vars = c("Dim.1", "Dim.2"))+
  labs(color="cluster") +
  theme_minimal()
```

The clustering partion now is more coherent with the class cylinders. 
On the left we have cluster two containing all and only records belonging to first class (cylinders = 4).
In the middle, cluster three contains both elements from class two (cylinders = 6) and three (cylinders = 8), while cluster one surrounds the remaing elements, all belonging to class three. Notice that the cluster division looks better also because the classes are already separated in space, according to the first two dimesions we are plotting.



#### LUMP and SPLIT errors

In order to evaluate the clustering partition, we use the two proposed metrics, lump and split errors, which count the number of time two elements of different classes are found in the same cluster, and how many objects of same class will be splitted in different clusters, respectively.

#####function definition

```{r}
count.errors <- function (class, cluster, error) {
  
  # two auxiliary matrices, used for building the "adiacence" matrix
  matrix.class.row <- matrix(class,length(class),length(class), byrow = T)
  matrix.class.col <- matrix(class,length(class),length(class), byrow = F)
  # the "adiacence" matrix: each cell is True if the record at row and column number belong to the same class.
  # of course it is symmetric, and the cells in the diagonal are all True (each element belong to the same class of itself)
  matrix.class.same <- matrix.class.col==matrix.class.row
  
  
  # we do the same for clustering partition
  matrix.cluster.row <- matrix(cluster,length(cluster),length(cluster), byrow = T)
  matrix.cluster.col <- matrix(cluster,length(cluster),length(cluster), byrow = F)
  matrix.cluster.same <- matrix.cluster.col==matrix.cluster.row
  
  if (error == "lump") {
    # a lump error is found when two element of different classes are putted in the same cluster
    matrix.lump <- matrix.cluster.same==T & matrix.class.same==F
    # since the matrix is symmetric, we need to consider half of the errors.
    n.lump <- sum(matrix.lump==T) / 2
    return(n.lump)
  } else { 
    if (error == "split") {
      # a split error is found when two element of same class are putted in two different clusters
      matrix.split <- matrix.class.same==T & matrix.cluster.same==F
      n.split <- sum(matrix.split==T) / 2
      return(n.split)
    } else {
      print (" USAGE: in error specify either \"lump\" or \"split\" ")
    }
  }
}

```

##### lump and spit errors for class Origins and Cylinders

```{r}
cat( "lump error with class origins: ", count.errors(cars$origin, car5.k3$cluster, "lump"), "\n")
cat( "split error with class origins: ", count.errors(cars$origin, car5.k3$cluster, "split"), "\n")
cat( "lump error with class cylinders: ", count.errors(cars_98$cylinders, car5_98.k3$cluster, "lump"), "\n")
cat( "split error with class cylinders: ", count.errors(cars_98$cylinders, car5_98.k3$cluster, "split"), "\n")
```
It's quite clear that in term of these two measure, lump and split errors, the clustering method represent better the class cylinders than the class origins.

We can notice an high splitting error related to the class `origins`, this means that a lot of records belonging to the same class have been assinged to different clusters. Such high error is due to the fact that records from class one, has been splitted between all the three clusters.
